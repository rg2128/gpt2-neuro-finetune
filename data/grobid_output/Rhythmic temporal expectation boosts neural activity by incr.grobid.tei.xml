<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rhythmic Temporal Expectation Boosts Neural Activity by Increasing Neural Gain</title>
				<funder ref="#_mvMM8YW">
					<orgName type="full">Wellcome Trust</orgName>
				</funder>
				<funder ref="#_vgzxQyb">
					<orgName type="full">European Commission</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Society for Neuroscience</publisher>
				<availability status="unknown"><p>Copyright Society for Neuroscience</p>
				</availability>
				<date type="published" when="2019-10-29">2019-10-29</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ryszard</forename><surname>Auksztulewicz</surname></persName>
							<idno type="ORCID">0000-0001-9078-3667</idno>
						</author>
						<author>
							<persName><forename type="first">Nicholas</forename><forename type="middle">E</forename><surname>Myers</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Experimental Psychology</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<postCode>OX2 6GG</postCode>
									<settlement>Oxford</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Oxford Centre for Human Brain Activity</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<postCode>OX3 7JX</postCode>
									<settlement>Oxford</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><forename type="middle">W</forename><surname>Schnupp</surname></persName>
							<idno type="ORCID">0000-0002-2604-0057</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Sciences</orgName>
								<orgName type="institution" key="instit1">City University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">Hong Kong Special Administrative Region of the People&apos;s Republic of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anna</forename><forename type="middle">C</forename><surname>Nobre</surname></persName>
							<idno type="ORCID">0000-0001-5762-2802</idno>
						</author>
						<title level="a" type="main">Rhythmic Temporal Expectation Boosts Neural Activity by Increasing Neural Gain</title>
					</analytic>
					<monogr>
						<title level="j" type="main">The Journal of Neuroscience</title>
						<title level="j" type="abbrev">J. Neurosci.</title>
						<idno type="ISSN">0270-6474</idno>
						<idno type="eISSN">1529-2401</idno>
						<imprint>
							<publisher>Society for Neuroscience</publisher>
							<biblScope unit="volume">39</biblScope>
							<biblScope unit="issue">49</biblScope>
							<biblScope unit="page" from="9806" to="9817"/>
							<date type="published" when="2019-10-29" />
						</imprint>
					</monogr>
					<idno type="MD5">2FCF30AFCD6FD8EC5FE8F9E979DFBE17</idno>
					<idno type="DOI">10.1523/jneurosci.0925-19.2019</idno>
					<note type="submission">Received April 24, 2019; revised Sept. 12, 2019; accepted Sept. 19, 2019.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-04-21T20:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>auditory processing</term>
					<term>magnetoencephalography</term>
					<term>multivariate decoding</term>
					<term>rhythm processing</term>
					<term>sensory prediction</term>
					<term>temporal orienting</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s>Temporal orienting improves sensory processing, akin to other top-down biases.</s><s>However, it is unknown whether these improvements reflect increased neural gain to any stimuli presented at expected time points, or specific tuning to task-relevant stimulus aspects.</s><s>Furthermore, while other top-down biases are selective, the extent of trade-offs across time is less well characterized.</s><s>Here, we tested whether gain and/or tuning of auditory frequency processing in humans is modulated by rhythmic temporal expectations, and whether these modulations are specific to time points relevant for task performance.</s><s>Healthy participants (N ϭ 23) of either sex performed an auditory discrimination task while their brain activity was measured using magnetoencephalography/electroencephalography (M/EEG).</s><s>Acoustic stimulation consisted of sequences of brief distractors interspersed with targets, presented in a rhythmic or jittered way.</s><s>Target rhythmicity not only improved behavioral discrimination accuracy and M/EEG-based decoding of targets, but also of irrelevant distractors preceding these targets.</s><s>To explain this finding in terms of increased sensitivity and/or sharpened tuning to auditory frequency, we estimated tuning curves based on M/EEG decoding results, with separate parameters describing gain and sharpness.</s><s>The effect of rhythmic expectation on distractor decoding was linked to gain increase only, suggesting increased neural sensitivity to any stimuli presented at relevant time points.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p><s>As our brains receive multiple sensory inputs over time, predicting when relevant events may happen can optimize perception and action <ref type="bibr" target="#b43">(Nobre and van Ede, 2018)</ref>.</s><s>The behavioral and neural enhancement effects of temporal expectation are likely due to a time-specific increase in neural excitability coinciding with the expected target onset <ref type="bibr" target="#b48">(Praamstra et al., 2006;</ref><ref type="bibr">Rohenkohl and Nobre, 2011;</ref><ref type="bibr" target="#b68">Zanto et al., 2011;</ref><ref type="bibr" target="#b53">Rohenkohl et al., 2012)</ref>.</s><s>In the context of rhythmic temporal expectation, these dynamic gain modulation effects have led to the hypothesis of neural entrainment, or phase alignment of ongoing neural activity to external rhythms.</s><s>Invasive studies showed that attention to one of two rhythmic streams, presented in parallel, aligns the excitability peaks in primary cortical regions to the expected event onsets in the attended stream <ref type="bibr" target="#b29">(Lakatos et al., 2008</ref><ref type="bibr" target="#b30">(Lakatos et al., , 2013))</ref>.</s><s>Similar effects associated with neural entrainment have been observed in noninvasive human studies using electroencephalography (EEG) and magnetoencephalography (MEG; <ref type="bibr" target="#b58">Stefanics et al., 2010;</ref><ref type="bibr" target="#b10">Cravo et al., 2013;</ref><ref type="bibr" target="#b18">Henry et al., 2014;</ref><ref type="bibr" target="#b9">Costa-Faidella et al., 2017;</ref><ref type="bibr" target="#b61">Ten Oever et al., 2017</ref>; but see <ref type="bibr" target="#b4">Breska and Deouell, 2017)</ref>.</s></p><p><s>However, it is unclear to what extent these rhythmic gain increases are target specific.</s><s>First, it is unknown whether rhythmic expectations adaptively adjust gain due to temporal trade-offs, upregulating neural sensitivity to expected stimuli but competitively downregulating the neural processing of events occurring earlier or later.</s><s>A recent behavioral study suggested that temporal cues enhance visual target processing at expected time points at the cost of unexpected time points <ref type="bibr" target="#b13">(Denison et al., 2017)</ref>, but whether rhythmic gain modulation operates in a similarly competitive manner, impairing neural processing at irrelevant phases of rhythmic stimulus streams relative to contexts in which no temporal expectation can be established, is an important open question, especially in light of a recently demonstrated double dissociation between temporal expectations based on rhythms versus specific intervals <ref type="bibr" target="#b5">(Breska and Ivry, 2018)</ref>.</s></p><p><s>Second, it is unclear whether rhythmic modulation of excitability is specific to relevant target features (akin to sharpened tuning of neural populations processing discriminant features), or nonspecific (i.e., also enhancing the processing of irrelevant distractors occurring in temporal proximity to targets, consistent with a true gain effect).</s><s>Modeling of behavioral responses to visual targets presented under different kinds of attention has suggested that spatial and feature-based attention rely on gain and tuning mechanisms to a different extent <ref type="bibr" target="#b33">(Ling et al., 2009)</ref>.</s><s>In the auditory modality, sustained attention to auditory rhythms <ref type="bibr" target="#b30">(Lakatos et al., 2013;</ref><ref type="bibr" target="#b45">O'Connell et al., 2014)</ref> and gradually increasing temporal expectation <ref type="bibr" target="#b24">(Jaramillo and Zador, 2011)</ref> sharpen frequency tuning (i.e., boost neural responses to the preferred acoustic frequency but dampen responses to other frequencies).</s><s>However, it is unclear whether the same holds for rhythmic temporal orienting in more complex streams where distractors and targets cannot be easily separated by their frequency contents.</s><s>In this case, both increased gain and sharpened tuning may provide plausible mechanisms of increasing sensory precision leading to improved processing of task-relevant features.</s></p><p><s>Time-specific modulation of sensory processing can be measured as changes in the quality of stimulus information encoded in neural signals.</s><s>Multivariate decoding of electrophysiological data provides useful tools for quantifying the dynamic modulation of stimulus-related information <ref type="bibr" target="#b14">(Garcia et al., 2013)</ref>, as well as in the context of temporal expectation <ref type="bibr" target="#b41">(Myers et al., 2015;</ref><ref type="bibr" target="#b63">van Ede et al., 2018)</ref>.</s><s>Here, we used multivariate decoding of MEG/ EEG responses to examine how processing auditory targets (tone chords), and distractors (pure tones) presented at variable intervals, are modulated by rhythmic temporal expectation.</s><s>The auditory modality was chosen as a natural testing ground for the mechanisms of neural alignment to rhythmic stimulus sequences <ref type="bibr" target="#b44">(Obleser et al., 2017;</ref><ref type="bibr" target="#b69">Zoefel and VanRullen, 2017)</ref>.</s><s>We used a model of population tuning, with separate parameters coding for the gain and sharpness of auditory frequency decoding, and tested whether temporal expectation modulates the processing in a specific way (sharpening the tuning of frequencies useful for discriminating targets) or in a nonspecific way (adjusting the gain of all frequencies).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participant sample</head><p><s>Healthy volunteers (N ϭ 23, 12 female; mean age, 27.8 years; age range, 18 -40 years) were invited to participate in the experiment upon written informed consent.</s><s>All participants had normal hearing, no history of neurological or psychiatric diseases, and normal or corrected-to-normal vision.</s><s>With the exception of one ambidextrous participant, all remaining participants were right handed by self-report.</s><s>The experimental procedures were conducted in accordance with the Declaration of <ref type="bibr">Helsinki (1991)</ref> and approved by the local ethics committee.</s><s>One participant withdrew from the study before completing the experimental session, and their incomplete data were discarded from analysis, so that data from 22 participants were included in the analysis.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental design and statistical analysis</head><p><s>Behavioral paradigm and stimulus design.</s><s>Participants were instructed to listen to an acoustic stream comprising sequences of pure tones interleaved with chords (Fig. <ref type="figure" target="#fig_0">1 A,</ref><ref type="figure">B</ref>).</s><s>Each pure tone had a carrier frequency drawn randomly with replacement from a set of 15 logarithmically spaced frequencies spanning two octaves (range, 460 -1840 Hz) and a duration drawn randomly with replacement from a set of five possible durations (23-43 ms in steps of 5 ms).</s><s>The tones were tapered with a Hanning window (5 ms rise/fall time) and formed otherwise gapless sequences of spectrally and temporally nonoverlapping stimuli interspersed by chord stimuli.</s></p><p><s>Chords comprised 6 of the 15 frequencies used for the tones.</s><s>Each chord was of one of two possible "types," A, and B, depending on their spectral profile.</s><s>Two of the six constituent tone amplitudes for the A and B tones were identical ("common"), while the remaining four amplitudes differed between A and B ("discriminant": two with amplitudes higher for each chord; Fig. <ref type="figure" target="#fig_0">1C</ref>, example).</s><s>The six frequencies making up the chords were chosen pseudorandomly for each participant.</s><s>Frequencies were chosen such that the chords could not be distinguished simply by overall pitch (i.e., the two discriminant frequencies with a larger amplitude in chord A were never both higher or lower than the other two discriminant frequencies).</s><s>The remaining nine frequency bands that were not part of the chord could be divided into those "adjacent" to versus "distant" from the discriminant frequencies.</s><s>The amplitude of each pure tone and chord was normalized by its mean loudness over time <ref type="bibr" target="#b15">(Glasberg and Moore, 2002)</ref> to render the loudness of each stimulus in the sequence constant.</s></p><p><s>While most chord durations were drawn from the same set as for pure tones (23-43 ms), a subset of chords (20%) was markedly longer (165 ms) and constituted "targets."</s><s>The participants were instructed to listen for these target chords and to indicate quickly whenever they heard a long A or a B chord.</s><s>In each trial, targets were presented after a sequence of pure tones interspersed with three to five short chords, and, upon hearing a longer chord, participants were asked to press one of two buttons (using their right index and middle fingers) assigned to chords A and B, respectively.</s><s>Button assignment was counterbalanced across participants.</s><s>Tone sequences continued for 715 Ϯ 10 ms (mean Ϯ SD) following target onset, including a 200 ms fadeout.</s><s>The entire sequence duration ranged between 3.846 and 7.742 s with no difference in duration across conditions (mean Ϯ SD; sequence duration, 5.683 Ϯ 0.818 vs 5.676 Ϯ 0.891 s in the rhythmic and jittered blocks, respectively).</s><s>While performing the task, participants were instructed to maintain fixation on a centrally presented yellow fixation cross the color of which changed to green (red) following correct (incorrect) responses.</s><s>Following feedback, a new trial started after 500 Ϯ 100 ms (i.e., mean Ϯ jitter) of fixation.</s><s>In addition to the fixation cross, participants viewed silent grayscale videos of semistatic landscapes that were irrelevant to the task; these videos were displayed to prevent fatigue due to prolonged fixation on an otherwise empty screen.</s><s>All visual stimulation was delivered using a projector (60 Hz refresh rate) in the experimenter room and transmitted to the MEG suite using a system of mirrors onto a screen located ϳ90 cm from the participants.</s></p><p><s>In separate blocks, chords formed either a rhythmic sequence [with each two chords separated by a constant interstimulus interval (ISI) of 1 s] or a jittered sequence (with 50% of the ISIs equal to 1 s, 25% of the ISIs drawn randomly from 570 to 908 ms, and 25% drawn randomly from 1092 to 1430 ms).</s><s>Each block contained 60 trials (targets) and 240 short chords.</s><s>Our analysis focused completely on the chords preceded by an ISI of 1 s, so that any behavioral or neural differences between rhythmic and jittered blocks were not due to physical differences in stimuli presented immediately before a given chord.</s><s>To obtain equal numbers of samples for the jittered and rhythmic conditions, each participant completed six blocks of target discrimination in jittered sequences and three blocks in rhythmic sequences.</s><s>Block duration was kept constant across the two conditions.</s><s>Block order was randomized per participant.</s><s>Participants were not briefed on the ISI distribution between the rhythmic and jittered conditions.</s></p><p><s>Before performing the task, participants were familiarized with the stimuli.</s><s>First, they heard 30 examples of each chord (A and B) in a randomized order, whereby A and B each contained two common frequencies and two discriminant frequencies at maximum amplitude.</s><s>Next, they performed a training block of the chord discrimination task (using a jittered sequence) in which the relative amplitude of discriminant frequencies between chords A and B was adjusted (using a one up, two down staircase procedure with an adaptive step size) to ϳ70% discrimination accuracy.</s><s>Following the training session, task stimuli (including chords with individually adjusted amplitude of discriminant frequencies) were rendered off-line and stored as 16 bit .wav</s><s>files at 48 kHz, delivered to the subjects' ears with tube ear phones and presented at a comfortable listening level (self-adjusted by each listener).</s><s>The stimulus set was generated anew for each participant.</s></p><p><s>Neural data acquisition.</s><s>Each participant completed one session of concurrent EEG and MEG recording lasting ϳ1 h for the entire experiment, excluding preparation.</s><s>Participants were comfortably seated in the MEG scanner in a magnetically shielded room.</s><s>MEG signals were acquired using a whole-head VectorView System (204 planar gradiometers, 102 magnetometers; Neuromag Oy, Elekta), sampled at a rate of 1 kHz and on-line bandpass filtered between 0.03 and 300 Hz.</s><s>The participant's head position inside the scanner was continuously tracked using head position index coils placed at four distributed points on the scalp.</s><s>Vertical electrooculogram (EOG) electrodes were placed above and below the right eye.</s><s>Additionally, eye movements and pupil size were monitored using a remote infrared eye-tracker (sampling both eyes at 1 kHz and controlled via Psychophysics Toolbox; EyeLink 1000, SR Research; <ref type="bibr" target="#b7">Cornelissen et al., 2002)</ref>.</s><s>Electrocardiogram (EKG) electrodes were placed on both wrists.</s><s>EEG data were collected using 60 channels distributed across the scalp according to the international 10-10 positioning system at a sampling rate of 1 kHz.</s></p><p><s>Behavioral data analysis.</s><s>Behavioral responses to targets were analyzed with respect to their accuracy (percentage correct responses), sensitivity (dЈ), criterion, and reaction times (RTs).</s><s>For each participant, trials with RTs longer than the individual median RT ϩ 2 SDs were excluded from analysis.</s><s>In the behavioral analyses, all responses were averaged in the rhythmic condition, while in the jittered condition only responses to targets preceded by an ISI of 1 s were taken into analysis to ensure that targets are preceded by the same ISI across conditions.</s><s>Mean accuracy A subset of these chords (20%) had a markedly longer duration and constituted targets.</s><s>Upon hearing a target, participants were asked to categorize it as one of two predefined categories ("a" or "b") using a button press.</s><s>Sequences were presented in blocks of two experimental conditions, as follows: in the rhythmic condition, chords were presented with a fixed ISI of 1 s, and participants could form a temporal expectation of when to expect each upcoming chord.</s><s>In the jittered condition, half of the ISIs, chosen at random were fixed at 1 s, and the remaining half ranged between 0.5 and 1.5 s, making chord onset unpredictable.</s><s>B, Spectrograms of example trials including pure tones surrounding the chords.</s><s>C, Chords were composed of eight pure tones each: four discriminant frequencies (two with a higher amplitude for each chord) and four common frequencies with equal amplitude for both chords.</s><s>Pure tones were drawn from a larger set of 15 frequencies (range, 460 -1840 Hz), including frequencies constituting the chords and other frequencies not included in the chords (adjacent to the discriminant frequencies or distant from them).</s><s>D, E, Temporal expectation increased the participants' behavioral sensitivity (dЈ values) in the chord discrimination task (marked by asterisk), but did not significantly affect their reaction times.</s><s>Bars represent population means; solid (dashed) lines represent individual participants' data consistent (inconsistent) with the direction of the group effect; error bars denote the SEM.</s><s>n.s., not significant.</s></p><p><s>and RT data were subject to separate paired t tests and compared between the rhythmic and jittered conditions.</s></p><p><s>Neural data preprocessing.</s><s>The SPM12 toolbox (Wellcome Trust Centre for Neuroimaging, University College London, London, U.K.) for Matlab (MathWorks) was used to perform all preprocessing steps.</s><s>Continuous M/EEG data were high-pass filtered at 0.1 Hz, notch filtered at 50 Hz and harmonics, and low-pass filtered at 200 Hz (all filters: fifthorder zero-phase Butterworth filters).</s><s>Different channel types (EEG electrodes, MEG gradiometers, and magnetometers) were preprocessed together.</s><s>Blink artifact correction was performed by detecting eye blink events in the vertical EOG channel and subtracting their two principal modes from the sensor data <ref type="bibr" target="#b23">(Ille et al., 2002)</ref>.</s><s>Similarly, heart beats were detected in the EKG channel, and their two principal modes were subtracted from sensor data.</s><s>EEG data (but not MEG data) were rereferenced to the average of all scalp channels.</s></p><p><s>Neural correlations with pure tone frequency.</s><s>To establish a basis for multivariate decoding of tone frequency from M/EEG data, we first tested whether M/EEG amplitude correlates with tone frequency in a mass-univariate way, and whether any such correlations can be source localized to auditory regions.</s><s>Our rationale was that, given the short ISIs between the tones (ϳ33 ms), auditory frequency decoding would rest on the amplitude on relatively early-latency M/EEG signals likely arising from tonotopically organized regions <ref type="bibr" target="#b59">(Su et al., 2014)</ref>; consequently, different dipole orientations associated with neural activity evoked by different tone frequencies should translate into systematic variability in M/EEG amplitude.</s><s>To test whether M/EEG amplitude covaries with pure tone frequency, we epoched M/EEG data from 200 ms before to 400 ms after each pure tone onset.</s><s>The epochs were averaged for each tone frequency and smoothed with a 20 ms moving average window.</s><s>The smoothing applied an effective low-pass frequency cutoff at ϳ20 Hz, implemented to ensure that the time series of M/EEG activity evoked by each given tone are not dominated by sharp peaks of responses evoked by consecutive tones presented at ISI rates of ϳ23-43 Hz.</s><s>M/EEG time series smoothing has also been shown to improve subsequent decoding accuracy <ref type="bibr" target="#b16">(Grootswagers et al., 2017)</ref>.</s><s>This resulted in 15 time series of mean M/EEG amplitude per participant and channel.</s><s>Spearman's rankorder correlation coefficients were calculated per participant, channel, and time point between the mean M/EEG amplitude and tone frequency (specifically, with a monotonic vector in which the lowest frequency was assigned the lowest value and the highest frequency to the highest value).</s><s>Spearman's rank-order correlation coefficients were chosen as they capture any monotonic relation between variables.</s><s>To establish whether different M/EEG channel types (EEG electrodes, MEG magnetometers, and planar gradiometers) contain signals sensitive to the frequency of presented pure tones, the grand-average channel-by-time matrices of correlation coefficients between M/EEG amplitude and tone frequency were decomposed into principal modes using singular value decomposition.</s><s>Per channel type, a set of principal modes (EEG: 7 modes of 60 original channels; magnetometers: 7 modes of 102 original channels; gradiometers: 11 modes of 204 original channels) explaining Ͼ95% of the original variance was retained.</s><s>This form of principal component analysis-based data dimensionality reduction has been shown to substantially improve the accuracy of M/EEG multivariate decoding <ref type="bibr" target="#b16">(Grootswagers et al., 2017)</ref>, used in subsequent analysis steps (see below).</s><s>The corresponding component weights were applied to individual participants' channel-by-time coefficient matrices and averaged.</s><s>The resulting time series-effectively summarizing the individual participants' correlation time series across channels-were analyzed using cluster-based permutation tests <ref type="bibr" target="#b37">(Maris and Oostenveld, 2007)</ref>, which are an established method of analyzing M/EEG data, without making any assumptions about the normality of data distribution, while correcting for multiple comparison over time.</s><s>Specifically, single-participant data were entered per channel type into separate cluster-based permutation one-sample t tests (which do not rely on any assumptions about the underlying data distribution), while correcting for multiple comparisons over time at a cluster-based threshold ( p Ͻ 0.05).</s></p><p><s>While several other studies found monotonic effects on EEG amplitude (especially for frequencies Ͼ500 Hz) at both early (tens of milliseconds: Tabachnick and Toscano, 2018) and late (hundreds of milliseconds <ref type="bibr" target="#b47">: Picton et al., 1978)</ref> latencies, nonmonotonic effects of tone frequency on EEG amplitude have also been reported (e.g., a quadratic relationship between tone frequency and N1 amplitude: <ref type="bibr">Herrmann et al., 2013a)</ref>.</s><s>Although a visual inspection of our data suggested a primarily monotonic relationship between tone frequency and M/EEG amplitude (Fig. <ref type="figure" target="#fig_1">2D</ref>), we have also tested for quadratic effects in the data.</s><s>To this end, we have repeated the analysis described above, this time correlating M/EEG amplitudes with a vector representing the frequency axis quadratically (whereby the lowest and highest frequencies were assigned the highest value, and the medium frequency the lowest value).</s><s>The remaining steps (principal component analysis and cluster-based permutation tests of correlation coefficient time series) were identical to that described above.</s></p><p><s>The time window in which we identified significant correlations between M/EEG amplitude and tone frequency was used for subsequent source reconstruction.</s><s>Specifically, individual participants' channel-bytime correlation coefficient time series (for all channel types) were projected into source space using the multiple sparse priors algorithm under group constraints <ref type="bibr" target="#b34">(Litvak and Friston, 2008)</ref>, as implemented in SPM12; the group constraints ensure that responses are reconstructed in the same subset of sources for the entire participant sample.</s><s>MEG and EEG data were source localized using a single generative model, which assumes that signals from different channel types arise from the same underlying current sources but map onto the sensors through different forward models (MEG, single shell; EEG, boundary element model), which also account for differences in units across data modalities <ref type="bibr" target="#b19">(Henson et al., 2009)</ref>.</s><s>Source activity maps were smoothed in 3D with a Gaussian kernel at FWHM of 8 mm and tested for statistical significance in paired t tests between each participant's estimated sources [for the 26 -126 ms time window (i.e., within a 100 ms time window around the correlation peak of 76 ms) for all channel types; see Results] and the corresponding prestimulus baseline.</s><s>The reason for this time window selection was that, in source reconstruction using multiple sparse priors, it is usually recommended to include rise and fall times of signals peaking at a specific latency, since sources of activity are estimated based on signal variance across time rather than mere amplitude differences between channels at a specific time point <ref type="bibr" target="#b35">(Lo ´pez et al., 2014)</ref>.</s><s>The resulting statistical parametric maps were thresholded at a peak-level uncorrected p value of Ͻ0.001 and corrected for multiple comparisons across voxels using a familywise error rate (FWE) of 0.05 under random field theory assumptions <ref type="bibr" target="#b26">(Kilner et al., 2005)</ref>.</s><s>Sources were assigned probabilistic anatomical labels using a Neuromorphometrics atlas implemented in SPM12.</s></p><p><s>Finally, to plot tone-evoked and chord-evoked responses [eventrelated potentials (ERPs) and event-related fields (ERFs)] in the time domain, continuous M/EEG data were subject to singular value decomposition, as described above.</s><s>Per participant, the principal components explaining Ͼ95% of the original variance were summarized and plotted over time in Figure <ref type="figure" target="#fig_1">2, D</ref> and<ref type="figure">E</ref>.</s></p><p><s>Phase locking to rhythmic stimulus structure.</s><s>To test whether rhythmic presentation of chords influenced ongoing low-frequency activity, we quantified the phase-locking value (PLV; <ref type="bibr" target="#b28">Lachaux et al., 1999)</ref> at chord onset.</s><s>Since we were primarily interested in PLV at low frequencies including 1 Hz, we calculated instantaneous power and phase of ongoing activity in the 0.5-5 Hz range (in 0.1 Hz steps) at each time point from Ϫ500 to 500 ms (in 50 ms steps) relative to chord onset using a Morlet wavelet transform with a fixed time window of 2000 ms for each timefrequency estimate.</s><s>To control for physical differences in stimulation between rhythmic and jittered blocks, we took into the analysis only these chords that were preceded and followed by an ISI of 1000 ms.</s><s>By this criterion, the first chord was excluded in each trial, as any temporal expectation could only be established after its presentation.</s><s>Based on the extracted phase values, per participant, channel, and condition (rhythmic vs jittered), we calculated PLV for each time-frequency point according to the following equation, where is a single-trial instantaneous phase of the wavelet transform, calculated for each of N trials, as follows:</s></p><formula xml:id="formula_0">PLV ϭ ͯ 1 N jϭ1 N e i ͯ.</formula><p><s>Given that PLVs are bound between 0 and 1, we used the (paired, twotailed) nonparametric test to assess whether phase locking is significantly different between the rhythmic and jittered conditions.</s><s>To control for multiple comparisons across channels and time-frequency points, we used cluster-based permutation tests as implemented in Fieldtrip.</s><s>The tests were conducted for each channel type (EEG electrodes, MEG magnetometers, and planar gradiometers) separately.</s></p><p><s>To ensure that the PLV analysis reveals effects that are not simply explained by differences in the amplitudes of ERPs/ERFs, we also extracted power estimates for each channel and time-frequency point and entered them into paired nonparametric cluster-based permutation tests, as described above.</s></p><p><s>Decoding pure tone frequency.</s><s>To quantify population-level gain and tuning of neural responses to acoustic inputs, we used M/EEG-based decoding of pure tone frequency (Fig. <ref type="figure" target="#fig_2">3 A,</ref><ref type="figure">B</ref>).</s><s>The decoding methods are based on previous work in decoding continuous features (e.g., visual orientation) from M/EEG signals <ref type="bibr" target="#b41">(Myers et al., 2015;</ref><ref type="bibr" target="#b65">Wolff et al., 2017;</ref><ref type="bibr" target="#b63">van Ede et al., 2018)</ref>, and additional preprocessing steps are based on a recent study <ref type="bibr" target="#b16">(Grootswagers et al., 2017)</ref> quantifying the effects of several analysis parameters on decoding accuracy, as detailed below; however, it should be noted that choices regarding optimal preprocessing and decoding methods are subject to an ongoing debate <ref type="bibr" target="#b16">(Guggenmos et al., 2018;</ref><ref type="bibr" target="#b27">Kriegeskorte and Douglas, 2019)</ref>.</s><s>In this analysis, we calculated the trialwise Mahalanobis distances <ref type="bibr" target="#b12">(De Maesschalck et al., 2000)</ref> of multivariate M/EEG signal amplitudes between the full range of pure tone frequencies and obtained frequency-by-frequency distance matrices, which were then parameterized in terms of gain and tuning <ref type="bibr" target="#b33">(Ling et al., 2009)</ref>.</s><s>First, we segmented the M/EEG data from all channels (principal components; see above) into separate trials, defined in relation to pure tones presented from 500 ms before to 500 ms after each (short) chord.</s><s>For instance, for tones presented 500 ms before a chord, we calculated (1) a vector of tone frequencies presented at this time point in each trial, and (2) a series of vectors of M/EEG amplitudes measured in the 26 -126 ms time window (in steps of 5 ms) after this time point in each trial.</s><s>The selected time window corresponded to the cluster in which a significant correlation between tone-evoked responses and tone frequency was observed (see Results).</s><s>In a leave-one-out cross-validation approach (optimal for M/EEG decoding; <ref type="bibr" target="#b16">Grootswagers et al., 2017)</ref>, per trial, we calculated 15 pairwise distances between M/EEG amplitudes observed in a given test trial and mean vectors of M/EEG amplitudes averaged for each of the 15 tone frequencies in the remaining trials.</s><s>The decision to perform our decoding analyses in a single-trial jack-knife approach is actually quite conservative, as calculating averages across a small number of trials during jack-knifing has been shown to further improve overall decoding <ref type="bibr" target="#b16">(Grootswagers et al., 2017)</ref>.</s><s>The Mahalanobis distances were computed using the shrinkage-estimator covariance calculated from all trials excluding the test trial <ref type="bibr" target="#b32">(Ledoit and Wolf, 2004)</ref>.</s><s>Although data from different channels (components) should in principle be orthogonal (given the previous dimensionality reduction using principal component analysis based on continuous data from the entire experiment) and see Materials and Methods) were estimated using data fused across channel types, and were inferred in the contrast between the time window of the observed correlations (26 -126 ms; chosen as a 100 ms time window around a peak correlation latency for all channel types) and the corresponding prestimulus baseline (126 -26 ms before tone onset).</s><s>All source estimates were significant at a threshold of p Ͻ 0.001 and correcting for multiple comparisons at a cluster level using an FWE-corrected p Ͻ 0.05.</s><s>Slices centered at 52, Ϫ48, and 6 mm in MNI coordinates.</s><s>D, Grand averages (shaded areas, SEM) of summarizedprincipalcomponentsoftone-evokedERP/ERFamplitudes,pertonefrequency(coloredlines)andchanneltype(panels).E,Grandaverages(shadedareas,SEM)ofsummarizedprincipalcomponents of chord-evoked ERP/ERF amplitudes, per condition (blue, rhythmic, red, jittered) and channel type (panels).</s><s>F, PLV: effect of rhythms (time-frequency maps).</s><s>Differences in PLV of M/EEG data at Ϫ500 to 500 ms relative to chord onset.</s><s>Each panel shows the time-frequency map of mean t statistics averaged across channels for a given channel type.</s><s>Contours outline the cluster of significant differences between rhythmic and jittered conditions, after correcting for multiple comparisons across channels and time-frequency points.</s><s>Asterisk: cluster-based p Ͻ 0.05; tilde: cluster-based p Ͻ 0.1.</s><s>G, PLV: effect of rhythms (topographic maps).</s><s>Each panel shows the topographical distribution of t statistic values at chord onset for the PLV estimate at 1 Hz.</s><s>Contours are as described above.</s></p><p><s>therefore warrant calculating Euclidean rather than Mahalanobis distance values, trialwise data may still retain useful (noise) covariance that may improve decoding.</s><s>Indeed, multivariate decoding based on Mahalanobis distance with Ledoit-Wolf shrinkage has been shown to outperform other correlation-based methods of measuring dissimilarity between brain states <ref type="bibr" target="#b2">(Bobadilla-Suarez et al., 2019)</ref>.</s><s>Mahalanobis distance-based decoding has also been shown to be more reliable and less biased than linear classifiers and simple correlation-based metrics <ref type="bibr" target="#b64">(Walther et al., 2016)</ref>.</s><s>Furthermore, rank correlation-based methods combined with data dimensionality reduction (e.g., in Mahalanobis distance calculation) have been shown to approach decoding accuracy achieved with linear discriminant analysis, Gaussian naive Bayes, and linear support vector machines <ref type="bibr" target="#b16">(Grootswagers et al., 2017)</ref>; thus, it is reasonable to assume that choosing Mahalanobis distance rather than rank correlation coefficient as a measure of neural dissimilarity further improves decoding accuracy, while at the same time being more computationally efficient than decoding based on other methods such as naive Bayes and support vector machines.</s></p><p><s>The minimum single-trial distance estimates observed in the 26 -126 ms time window were selected to accommodate frequency-dependent peak latencies of the middle-latency auditory evoked potential <ref type="bibr" target="#b66">(Woods et al., 1995)</ref>.</s><s>These distance estimates were then averaged across trials per tone frequency, resulting in a 15 ϫ 15 distance matrix for all tones presented, at the relevant time bin relative to chord onset.</s><s>This procedure was repeated for time bins relative to chord onset from 500 ms before to 500 ms after the chord, in steps of 10 ms.</s><s>As before, only trials in which chords were preceded by an ISI of 1 s were taken into the analysis, which was conducted separately for rhythmic and jittered blocks.</s><s>In this manner, we computed single-participant distance matrices for each time point relative to temporally predictable versus unpredictable chord presentation.</s></p><p><s>The quality of the decoding of pure tone frequency was assessed by comparing the estimated distance matrices with an "ideal decoding" distance matrix, with the lowest distance values along the diagonal and progressively higher distance values along the off-diagonal (Fig. <ref type="figure" target="#fig_2">3B</ref>).</s><s>To this end, for each participant and time point (from 500 ms before to 500 .</s><s>Each row consists of a vector of distances between the neural activity on the given trial and the average neural activity in response to each of the 15 frequencies (calculated from all other trials; i.e., the single-trial dissimilarity estimates between amplitudes measured for the tone frequency presented in a given trial and all other frequencies presented in the remaining trials).</s><s>Frequency-tuning matrices (right), summarizing the population-level tuning curves, were obtained after averaging across trials, per frequency, resulting in a 15 ϫ 15 similarity matrix between all tone frequencies (each row represents the distance of all test trials of a given frequency to the remaining trials sorted per frequency and is shown in columns).</s><s>The observed frequency-tuning matrices (top right, example from one participant) were Spearman correlated with the "ideal" tuning matrix (bottom right), which consisted of the difference (in Hz) between pairs of tone frequencies.</s><s>This correlation coefficient provided a summary statistic that reflects decoding quality (i.e., how closely the relative dissimilarity between tone-evoked neural responses; "observed" in the figure) corresponds to the relative dissimilarity between tone frequencies ("ideal" in the figure).</s><s>C, The observed grand average frequency-tuning matrix (averaged across participants, time points, and conditions).</s><s>D, Rank-order correlation coefficients between the estimated tuning and ideal tuning for each frequency (i.e., each row in the frequency-tuning matrix).</s><s>Single grey dots mark single participants; black dots mark mean across participants.</s><s>E, Frequency decoding was significantly enhanced (cluster-corrected p Ͻ 0.05; black bar) in the rhythmic (blue) versus jittered (red) blocks between Ϫ100 and Ϫ80 ms before chord presentation.</s><s>Gray box marks chord presentation latency, where no pure tones were presented and consequently no frequency decoding can be established.</s><s>Shaded areas mark SEM across participants.</s><s>F, G, Chord decoding was based on the same methods as in A and B, except single-trial Mahalanobis distances were calculated for same versus different chords (instead of 15 different distractor frequencies).</s><s>Only neural responses to short chords preceded by an ISI of 1 s were analyzed.</s><s>H, Chord decoding was significantly enhanced (cluster-corrected p Ͻ 0.05; black bar) in the rhythmic (blue) vs jittered (red) blocks between 115 and 136 ms following chord onset.</s><s>Shaded areas mark the SEM across participants.</s><s>Freq, Frequency.</s></p><p><s>ms after the expected chord onset), we calculated the Spearman's rank correlation coefficient between the estimated distance matrix and the ideal distance matrix.</s><s>Spearman's correlation coefficient was chosen to avoid making any assumptions about the shape of the ideal distance matrix (e.g., linear or log-spaced along the frequency axes), as it quantifies the strength of a monotonic relationship between two variables.</s><s>The resulting time series of correlation coefficients were entered into a cluster-based permutation paired t test between rhythmic and jittered conditions.</s><s>Time windows in which clusters of significant tests were observed were based on corrections for multiple comparisons over the entire time window (Ϫ500 to ϩ500 ms) at a cluster-based threshold of p Ͻ 0.05 (two-tailed).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural data analysis</head><p><s>Decoding chords.</s><s>In addition to decoding pure tone frequency from the trial segments ranging from Ϫ500 to ϩ500 ms relative to expected chord onset, we also decoded chord identity itself based on M/EEG data evoked by short chord presentation (Fig. <ref type="figure" target="#fig_2">3 F,</ref><ref type="figure">G</ref>).</s><s>The decoding methods were identical to those described above, except that instead of calculating pairwise distance values between a given trial and each of the 15 frequencies, we calculated pairwise distance values between a given (test) trial and (1) all remaining trials in which the same chord was presented as in the test trial as well as (2) all trials in which the other chord was presented.</s><s>The relative distance was quantified per trial by subtracting the distance to "same chord" trials from the distance to "other chord" trials and averaged across trials.</s><s>This procedure was repeated for each participant and time point from Ϫ100 to ϩ400 ms relative to chord onset, separately for rhythmic and jittered conditions.</s><s>Only chords preceded by an ISI 1 s were included in the analysis.</s><s>The resulting single-subject time series of chord-decoding accuracy were subject to cluster-based permutation statistics, as described above.</s></p><p><s>Gain/tuning model of frequency encoding.</s><s>To characterize the effects of rhythmic expectation on pure tone decoding in terms of gain and tuning to acoustic inputs, we fitted a simple model to individual participants' distance matrices, averaged across the time window in which significant results were observed (Ϫ100 to Ϫ80 ms before expected chord onset; see Results).</s><s>Specifically, for each participant and condition, we fitted a three-parameter model to the observed distance matrices Z, with free parameters describing the gain g (i.e., M/EEG distance independent of relative tone frequency ⌬f ), tuning (i.e., a sharper or broader distribution of distance values along the relative tone frequency axis), and a constant term, c (i.e., mean distance across all relative tone frequencies), as follows:</s></p><formula xml:id="formula_1">Z ϭ ge Ϫ⌬f 2 2 2 ϩ c.</formula><p><s>This model equation is based on previous modeling work in humans investigating the gain and tuning effects of top-down attention in the visual domain <ref type="bibr" target="#b33">(Ling et al., 2009)</ref>.</s><s>Figure <ref type="figure" target="#fig_3">4B</ref> depicts the effects of each of these parameters on overall decoding matrices.</s><s>Crucially, the gain parameter describes overall decoding quality (i.e., the relative similarity of neural responses to similar vs dissimilar frequencies, akin to nonspecific sensitivity modulation), while the tuning parameter describes the smoothness of the decoding matrix across the diagonal (i.e., the relative similarity of neural responses to identical vs adjacent frequencies, akin to frequency-specific sharpening).</s><s>The resulting decoding matrices were assumed to be symmetric along the diagonal, based on previous literature suggesting overall frequency symmetry in spectrotemporal receptive fields of neurons in auditory cortex <ref type="bibr" target="#b39">(Miller et al., 2002)</ref>.</s><s>All model fitting was performed using built-in Matlab robust fitting functions, with starting points based on the model fit to the grand average distance matrix (Fig. <ref type="figure" target="#fig_2">3C</ref>).</s><s>First, per participant, we fitted the full model with three free parameters, as well as a set of six reduced models in which each combination of the three parameters could be fixed to the value based on the model fit to the grand average distance matrix.</s><s>In total, seven models were fitted for each participants' distance matrix (averaged across conditions).</s><s>The models were compared using individual participants' Akaike information criterion (AIC) values, which reward models for their goodness of fit but penalize them for model complexity.</s><s>The AIC values were treated as an approximation to log-model evidence and entered into a formal Bayesian model selection, as implemented in SPM12 <ref type="bibr">(Peters et al., 2012)</ref>.</s><s>The winning model was then fitted per participant and condition, and the resulting parameter estimates were subject to three paired t tests (one per parameter) between fits to distance matrices estimated from rhythmic and jittered conditions.</s><s>The t tests were corrected for multiple comparisons using a Bonferroni correction.</s></p><p><s>In addition to testing the effects of rhythm on gain and tuning across all tone frequencies, we also considered the possibility that gain and/or tuning modulation might be specific for those tone frequencies that were diagnostic of chord discrimination (Fig. <ref type="figure" target="#fig_0">1C</ref>).</s><s>To this end, we repeated the model-fitting procedure described above, this time fitting the (full) models separately to the following four categories of tone frequencies:</s></p><p><s>(1) discriminant frequencies, whose amplitude differentiated between chords A and B; (2) frequencies adjacent to discriminant frequencies, which, however, do not constitute either chord A or B; (3) frequencies nonadjacent (distant) to discriminant frequencies, which do not belong to either chord A or B; and (4) frequencies common to chords A and B. The resulting parameter estimates were entered into a 2 ϫ 4 repeatedmeasures ANOVA with factors temporal expectation (rhythmic vs jittered) and tone frequency <ref type="bibr">(discriminant, adjacent, distant, common)</ref>.</s><s>We specifically tested for the interaction between the two factors, which would indicate that gain and/or tuning modulation by temporal expectation may depend on the type of tone frequency.</s></p><p><s>Finally, to test whether the effects of rhythm on tone (distractor) processing and chord (potential target) processing are interrelated, the following measures were contrasted between conditions (rhythmic vs jittered blocks), and the resulting differences were z scored and Pearson correlated across participants, as follows: (1) tone decoding (i.e., correlation coefficient with the ideal decoding matrix, averaged across the time window between Ϫ100 and Ϫ80 ms relative to chord onset); (2) the gain parameter of the gain/tuning model; (3) chord decoding (average Mahalanobis distance in the 115-136 ms postchord time window); and (4) behavioral accuracy.</s><s>Correlations between measures were Bonferronicorrected for multiple comparisons.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Behavioral results</head><p><s>Behavioral performance in a chord discrimination task was affected by the temporal predictability of the chords (Fig. <ref type="figure" target="#fig_0">1 D,</ref><ref type="figure">E</ref>).</s><s>Participants discriminated the target chords more accurately in the rhythmic blocks than in the jittered blocks (mean Ϯ SD: 72.04 Ϯ 15.82% in the rhythmic blocks; 68.63 Ϯ 16.07% in the jittered blocks; paired t test: t (21) ϭ 2.797, p ϭ 0.011).</s><s>This behavioral advantage was reflected in the participants' sensitivity dЈ (mean Ϯ SD: 0.944 Ϯ 0.809 in the rhythmic blocks; 0.785 Ϯ 0.782 in the jittered blocks; paired t test: t (21) ϭ 2.144, p ϭ 0.044).</s><s>There was no difference in criterion (mean Ϯ SD: Ϫ0.008 Ϯ 0.738 in the rhythmic blocks; 0.006 Ϯ 0.756 in the jittered blocks; paired t test t (21) ϭ Ϫ0.222, p ϭ 0.827).</s><s>Mean reaction times also did not differ between rhythmic and jittered blocks (mean Ϯ SD: 713 Ϯ 69 ms in the rhythmic blocks; 717 Ϯ 61 ms in the jittered blocks; paired t test: t (21) ϭ 0.751, p ϭ 0.461), although the overall slow mean reaction times indicate that some participants did not follow the instructions to respond as soon as possible upon hearing the target chords, and instead waited until the end of the tone sequence.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Activity in auditory regions covaries with tone frequency</head><p><s>To establish a basis for subsequent decoding, we tested whether tone frequency is reflected in evoked M/EEG signals.</s><s>M/EEG amplitudes were correlated with tone frequency for all sensor types (Fig. <ref type="figure" target="#fig_1">2A</ref>), as follows: for time series summarizing signal amplitudes obtained from MEG magnetometers (for details, see Materials and Methods), we observed a significant monotonic correlation between signal amplitude and tone frequency at 22-61 ms following tone onset (all t (21) within the cluster Ͼ2.133; clusterlevel p ϭ 0.002); for MEG gradiometers, significant correlations were observed at 28 -86 ms following tone onset (all t (21) within the cluster Ͼ2.079; cluster-level p ϭ 0.011); and finally, EEG amplitudes correlated with tone frequency at 48 -83 ms following tone onset (all t (21) within the cluster Ͼ2.087; cluster-level p ϭ 0.028).</s><s>Sensor topography of mean correlation coefficients are shown per channel type in Figure <ref type="figure" target="#fig_1">2B</ref>.</s><s>No significant clusters were observed for the analysis of quadratic effects of tone frequency on M/EEG amplitude (all clusters: p Ͼ 0.05).</s></p><p><s>Source reconstruction of the correlation coefficient time series, contrasting source-level activity estimates for the 100 ms time window around the latency (76 ms) at which the peak correlation between M/EEG amplitude and tone frequency has been observed (26 -126 ms) and the corresponding prestimulus baseline (Ϫ126 to Ϫ26 ms relative to tone onset) revealed two significant clusters of source-level activity (Fig. <ref type="figure" target="#fig_1">2C</ref>), encompassing bilateral primary auditory cortex (transverse temporal gyrus), planum temporale, and more lateral regions of superior temporal gyrus.</s><s>MNI (Montreal Neurological Institute) coordinates of peak voxels, the corresponding statistics, and anatomical labels are reported in Table <ref type="table" target="#tab_0">1</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rhythmic stimulus structure increases phase locking to chord onset</head><p><s>Rhythmic temporal expectation increased the low-frequency PLV at chord onset.</s><s>Increased phase locking was observed in EEG channels (28 of 60 channels; paired t test statistic peaking at 1 Hz, Ϫ50 ms relative to chord onset; cluster-level p ϭ 0.028).</s><s>A similar trend was observed in MEG magnetometers (37 of 102 channels; paired t test statistic peaking at 1.8 Hz, 0 ms relative to chord onset; cluster-level p ϭ 0.067; Fig. <ref type="figure" target="#fig_1">2 F,</ref><ref type="figure">G</ref>), encompassing the chord presentation rate of 1 Hz.</s><s>There was no accompanying increase in the power of ongoing activity for these or any other A, Grand average frequency-tuning matrices for the rhythmic and jittered blocks, respectively (averaged between Ϫ100 and Ϫ80 ms before chord onset; Fig. <ref type="figure" target="#fig_2">3E</ref>).</s><s>Blue colors correspond to low distance (i.e., high similarity).</s><s>B, Effects of varying each of three free parameters in the gain/tuning model.</s><s>The x-axis corresponds to the off-diagonal and the y-axis to the shading of a frequency-tuning matrix.</s><s>C, Model comparison of seven models (solid vs no outline, free vs fixed gain; orange vs gray, free vs fixed tuning; dark vs light, free vs fixed constant).</s><s>The winning (full) model significantly outperformed the remaining models (see Results).</s><s>D, Effects of temporal expectation on model parameters.</s><s>Only the gain parameter was significantly different (asterisk) between rhythmic and jittered contexts.</s><s>E, Correlation between the benefit in tone decoding (for rhythmic vs jittered blocks) and the difference in gain parameters (between rhythmic and jittered conditions) of the model.</s><s>Dashed/solid line is the correlation coefficient slope before/after excluding an outlier (empty circle).</s><s>F, Relative gain (for rhythmic vs jittered conditions) did not significantly differ between models estimated separately for different frequency types (Fig. <ref type="figure" target="#fig_0">1C</ref>).</s><s>G, The time course of the gain parameters for the entire analyzed time range (Ϫ500 to 500 ms relative to chord onset).</s><s>Shaded areas mark SEMs.</s><s>Blue, Rhythmic blocks; red, jittered blocks.</s><s>Outline marks the latency of a significant effect reported in D. Rel., Relative.</s><s>n.s., not significant; *p Ͻ 0.05 time-frequency points in the analyzed range (0.5-5 Hz, Ϫ1000 to 1000 ms relative to chord onset; all clusters p Ͼ 0.4), suggesting that the observed PLV increase is not merely due to power differences between conditions (van Diepen and Mazaheri, 2018).</s><s>No significant differences in either PLV or power estimates were observed for MEG planar gradiometers ( p Ͼ 0.1).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tone frequency can be decoded per time point relative to chord onset</head><p><s>Based on M/EEG amplitudes observed at all sensors, we calculated individual tone-by-tone Mahalanobis distance matrices, per time point, from Ϫ500 to ϩ500 ms relative to chord onset (see Materials and Methods).</s><s>Averaging across rhythmic and jittered blocks, the corresponding distance matrices showed significant above-chance tone frequency decoding for all inspected frequencies (Spearman's rank correlation coefficient between the observed distance matrix and a matrix representing ideal decoding; one-sample t test: all t (21) Ͼ 8.173, all p Ͻ 0.001; Fig. <ref type="figure" target="#fig_2">3D</ref>) and time points (all t (21) Ͼ 2.766, all p Ͻ 0.012).</s><s>Crucially, tone decoding was also influenced by rhythmic temporal expectation (Fig. <ref type="figure" target="#fig_2">3E</ref>).</s><s>Specifically, when testing for differences between tone decoding per time point in rhythmic versus jittered blocks, a significant effect of temporal expectation was identified in a time window ranging between Ϫ100 and Ϫ80 ms before chord onset (permutation-based paired t test: all t Ͼ 2.136, cluster p ϭ 0.016; Cohen's d ϭ 0.900).</s><s>In this time window, a higher correlation with the ideal decoding matrix was observed in rhythmic blocks (mean Ϯ SD: ϭ 0.215 Ϯ 0.173) than in the jittered blocks (mean Ϯ SD: ϭ 0.070 Ϯ 0.146).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chord decoding</head><p><s>In addition to establishing that pure tone frequency can be robustly decoded and identifying the effects of rhythmic expectation on processing tones presented at different latencies relative to chords, we also examined whether rhythmic expectation influences chord decoding itself.</s><s>To this end, we calculated relative Mahalanobis distance between M/EEG topographies of responses evoked by chord presentation.</s><s>A significant effect of rhythmic expectation was identified in the time window between 115 and 136 ms after chord onset (permutation-based paired t test between rhythmic and jittered blocks: all t Ͼ 2.099, cluster p ϭ 0.044; Cohen's d ϭ 0.783; Fig. <ref type="figure" target="#fig_2">3H</ref> ).</s><s>In this time window, chord decoding was enhanced in the rhythmic condition (mean Ϯ SD relative Mahalanobis distance ϭ 0.009 Ϯ 0.008), relative to the jittered condition (mean Ϯ SD relative Mahalanobis distance ϭ 0.001 Ϯ 0.011).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal expectation modulates gain of population-level frequency processing</head><p><s>Having identified the significant effect of temporal expectation on pure tone decoding, we sought to investigate whether this effect on tone processing is due to gain and/or tuning sharpness modulation.</s><s>To this end, we constructed and compared several alternative models of population-tuning curves that were fitted to the observed decoding matrices and parameterized them in terms of gain and tuning sharpness (Fig. <ref type="figure" target="#fig_3">4B</ref>).</s><s>First, we fitted a gain/tuning model with three free parameters (gain, tuning, constant)-as well as reduced models with different subsets of free parameters-to the observed decoding matrices (averaged across rhythmic and jittered blocks) in single participants.</s><s>Bayesian model comparison using single-participant AIC values as an approximation to log model evidence <ref type="bibr">(Peters et al., 2012)</ref> revealed that the full model outperformed the remaining models (Fig. <ref type="figure" target="#fig_3">4C</ref>), with expected model probability given the data p(m͉y) ϭ 0.74 (all remaining models Ͻ0.05) and exceedance probability of Ͼ99.9% that the full model is better than any reduced model in describing the overall tone-decoding matrices (averaged across conditions).</s></p><p><s>Next, to test whether temporal expectation influences gain and/or tuning, we refitted the full model separately to decoding matrices obtained in each condition (rhythmic vs jittered blocks; Fig. <ref type="figure" target="#fig_3">4A</ref>).</s><s>A comparison of the obtained parameter estimates (Fig. <ref type="figure" target="#fig_3">4D</ref>) revealed a significant effect of temporal expectation on the gain parameter (paired t test: t (21) ϭ Ϫ2.779, p ϭ 0.011; Cohen's d ϭ 0.783; please note that gain is expressed as a negative number, i.e., a more negative gain parameter corresponds to better decoding).</s><s>This effect was specific to the time range for which significantly improved decoding was observed in the rhythmic conditions (i.e., Ϫ100 to Ϫ80 ms relative to chord onset; Fig. <ref type="figure" target="#fig_3">4G</ref>).</s><s>Furthermore, the median peak latency of the gain effect calculated for each participant was Ϫ80 ms relative to chord onset, coinciding with the latency of the group-level effect.</s><s>Although the effect of experimental condition on the constant term was nominally significant, this test did not survive Bonferroni correction for multiple comparisons (t (21) ϭ 2.101, p ϭ 0.048).</s><s>The effect of rhythm on the tuning sharpness parameter was not significant (t (21) ϭ 1.039, p ϭ 0.310).</s></p><p><s>Further, we tested whether the effect of temporal expectation on the gain parameter might be driven by a specific class of tone frequencies, such as those discriminating between the two chords that needed to be categorized by the participants.</s><s>Thus, we repeated the model fitting for four classes of tones (discriminant, adjacent, distant, and common frequencies; for details, see Materials and Methods).</s><s>A repeated-measures ANOVA revealed a main effect of temporal expectation, as identified above (F (1,63) ϭ 10.111, p ϭ 0.004), but no main effect of frequency type (F (3,63) ϭ 2.253, p ϭ 0.091) and, crucially, no interaction between the two (F (3,63) ϭ 1.725, p ϭ 0.171).</s><s>Therefore, the effect of temporal expectation on gain did not depend on tone type (Fig. <ref type="figure" target="#fig_3">4F</ref> ).</s></p><p><s>Finally, we investigated whether the neural and behavioral benefits of temporal expectation are correlated.</s><s>Across participants, we correlated the z-scored differences between estimates of the following variables, obtained from the rhythmic and jittered condition, respectively, as follows: (1) gain parameter; (2) tone decoding (i.e., rank-order correlation with the ideal decoding matrix); (3) chord decoding (Mahalanobis distance); and (4) behavioral accuracy.</s><s>A significant correlation was observed between the effect of temporal expectation on the gain parameter and the underlying tone-decoding modulation by temporal expectation (r ϭ Ϫ0.549, p ϭ 0.008; significant after Bonferroni correction for multiple comparisons across pairs of variables; Fig. <ref type="figure" target="#fig_3">4E</ref>).</s><s>After removing one outlier participant whose data were characterized by the Cook's distance metric exceeding the mean, the correlation remained significant (r ϭ Ϫ0.511, p ϭ 0.018).</s><s>No other correlations were found to be significant.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p><s>We have shown that rhythmic temporal expectation improves target chord discrimination accuracy, increases the phase locking of neural signals at chord onset, as well as improves M/EEGbased chord decoding.</s><s>Interestingly, we also show that before chord (i.e., a potential target) onset, temporal expectation improves the decoding of irrelevant distractors (pure tones).</s><s>This beneficial effect can be modeled as increased gain to any stimuli (auditory frequencies) presented at time points adjacent to expected chord onset, and independent of whether processing these frequencies may be beneficial for chord discrimination.</s></p><p><s>In the present study, rhythm-induced temporal expectation increased the participants' sensitivity to target chords.</s><s>Similar behavioral improvements have been reported previously, typically accompanied by shorter RTs to expected targets <ref type="bibr" target="#b24">(Jaramillo and Zador, 2011;</ref><ref type="bibr" target="#b50">Rimmele et al., 2011;</ref><ref type="bibr" target="#b53">Rohenkohl et al., 2012;</ref><ref type="bibr" target="#b10">Cravo et al., 2013)</ref>.</s><s>While here we only compared responses to stimuli presented in rhythmic (isochronous) and jittered sequences while controlling for physical differences between conditions (i.e., only selecting targets preceded by identical intervals), other researchers have also found accuracy improvements in quasirhythmic sequences when acoustic targets were presented following a mean interval versus at other intervals (but see <ref type="bibr" target="#b25">Jones, 2015;</ref><ref type="bibr" target="#b22">Herrmann et al., 2016)</ref>.</s><s>Another recent study has shown that, while different types of temporal expectation might lead to accuracy benefits, rhythmic expectation specifically shortens RTs <ref type="bibr" target="#b40">(Morillon et al., 2016)</ref>.</s><s>However, RTs have been suggested to be more sensitive to temporal orienting in detection tasks than in discrimination tasks <ref type="bibr" target="#b8">(Correa et al., 2004)</ref>.</s><s>While in the present study participants were instructed to discriminate chords by responding as soon as possible after hearing a target, auditory streams continued for several hundreds of milliseconds following target offset, which may have resulted in overall slow responses (Fig. <ref type="figure" target="#fig_0">1E</ref>) and a reduced sensitivity to detect RT effects.</s></p><p><s>Beyond the increased behavioral sensitivity to target chords, we also observed improved neural decoding of short chords in the rhythmic versus jittered condition.</s><s>Previous auditory studies have shown that rhythmic presentation of targets presented at a low signal-to-noise ratio amid continuous distractors increases their detectability <ref type="bibr" target="#b31">(Lawrance et al., 2014;</ref><ref type="bibr" target="#b49">Rajendran et al., 2016)</ref>.</s><s>Similar findings in visual studies <ref type="bibr" target="#b61">(Ten Oever et al., 2017)</ref> have been linked to increased phase locking of neural activity around the expected target onset.</s><s>In our study, rhythm-induced temporal expectation increased phase locking of M/EEG signals at the chord presentation rate (but not chord-evoked ERF/ERP amplitude), which is consistent with previous reports <ref type="bibr" target="#b10">(Cravo et al., 2013;</ref><ref type="bibr" target="#b18">Henry et al., 2014;</ref><ref type="bibr" target="#b9">Costa-Faidella et al., 2017)</ref> and with the entrainment hypothesis <ref type="bibr" target="#b56">(Schroeder and Lakatos, 2009</ref>; for review, see <ref type="bibr" target="#b17">Haegens and Zion Golumbic, 2018)</ref>, which posits that external rhythms synchronize low-frequency neural activity and create time windows of increased sensitivity to stimuli presented at expected latencies.</s><s>However, since phase locking has been shown to also increase due to interval-based expectations <ref type="bibr" target="#b3">(Breska and Deouell, 2014)</ref>, it may not be a specific measure of rhythminduced temporal expectation.</s></p><p><s>In addition to improving the decoding of short chords (potential targets), rhythmic expectation also improved the decod-ing of pure tones (irrelevant distractors) preceding the chords.</s><s>Current hypotheses are largely agnostic to whether neural alignment to external rhythms also results in temporal trade-offs, creating windows of decreased sensitivity at unexpected or irrelevant latencies.</s><s>Such competitive effects have been described in the domain of spatial visual attention <ref type="bibr" target="#b6">(Carrasco, 2011)</ref>; however, temporal expectations have been suggested to play a largely modulatory role, amplifying the influence of other (e.g., spatial) sources of top-down control rather than themselves exerting strong influences on neural processing <ref type="bibr" target="#b54">(Rohenkohl et al., 2014)</ref>.</s><s>While processing limitations over time have long been established (e.g., in the attentional blink literature; <ref type="bibr" target="#b57">Shapiro et al., 1994)</ref>, temporal expectations can in fact prevent attentional blink: knowing when subsequent targets will occur can improve their processing and diminish the (detrimental) effects of preceding targets <ref type="bibr" target="#b38">(Martens and Johnson, 2005)</ref>.</s><s>Similarly, cues predicting target latency do seem not only to improve target processing but also to impair processing targets that appear at invalidly cued latencies <ref type="bibr" target="#b13">(Denison et al., 2017)</ref>.</s><s>In this study, however, we did not observe impaired processing of stimuli presented at unexpected time points, which would likely manifest as impaired decoding and lower gain in the rhythmic versus jittered condition several hundred milliseconds before and after chord onset.</s><s>Instead, our results suggest that while rhythmic auditory expectation increases sensitivity at expected latencies, it does not necessarily involve temporal trade-off with unexpected latencies.</s></p><p><s>We also considered another possible trade-off, namely temporal expectation boosting the processing of relevant targets at the expense of irrelevant distractors.</s><s>A recent EEG study showed that anticipatory cues not only boost visual target decoding, but also decrease its interference by distractors presented just after the targets, possibly reflecting a protective time window for target processing <ref type="bibr" target="#b63">(van Ede et al., 2018)</ref>.</s><s>However, as shown in other contexts <ref type="bibr">(Rohenkohl et al., 2011;</ref><ref type="bibr" target="#b40">Morillon et al., 2016;</ref><ref type="bibr" target="#b5">Breska and Ivry, 2018)</ref>, rhythm-induced expectations may not operate in the same manner as cue-induced expectations.</s><s>Indeed, rhythms can facilitate performance independently of whether they are predictive of when the relevant targets may appear <ref type="bibr" target="#b55">(Sanabria et al., 2011)</ref>.</s><s>In some cases, performance is superior for those targets that occur on-beat, even if targets more often occur off-beat <ref type="bibr" target="#b3">(Breska and Deouell, 2014)</ref>.</s><s>In line with the latter results, our findings show improved decoding of irrelevant stimuli if they are presented at latencies leading up to the expected onsets of potential targets.</s><s>While these differences were observed between Ϫ100 and Ϫ80 ms but not at even shorter latencies before chord onset, it is worth noting that decoding pure tones was based on M/EEG activity evoked by these tones (i.e., with a lag of up to 126 ms).</s><s>Thus, just before chord onset, interference between chordevoked activity and tone-evoked activity may have compromised tone decoding.</s><s>Given the previously observed differences between rhythm-and cue-induced temporal expectations, it remains an important open question whether the type of temporal expectation manipulations, and/or individual participants' strategies in generating these expectations, may influence the latencies at which improved decoding can be observed.</s></p><p><s>To interpret the finding that rhythm-based expectation improves the decoding of irrelevant distractors before the expected target onset, we used a model that independently parameterized the gain and tuning of population-level frequency coding and found that rhythm-based expectation increased the gain of pure tone decoding.</s><s>No evidence was found for the sharpening of tuning induced by temporal expectation.</s><s>This suggests that, unlike in previous (animal) studies showing that sustained attention to acoustic rhythms <ref type="bibr" target="#b45">(O'Connell et al., 2014)</ref> or increased target onset probability <ref type="bibr" target="#b24">(Jaramillo and Zador, 2011)</ref> sharpen frequency tuning, in the current study rhythm-induced expectations-at the level of population-based decoding-could be linked to dynamic modulations of gain, more akin to classical neuromodulatory effects <ref type="bibr" target="#b0">(Auksztulewicz et al., 2018)</ref>.</s><s>Previous behavioral modeling studies showed that rhythm-based expectation does indeed increase the signal-to-noise gain of sensory evidence in a visual discrimination task <ref type="bibr" target="#b53">(Rohenkohl et al., 2012)</ref>.</s><s>Here, the gain effect was independent of whether the specific frequencies were useful for discriminating potential targets, further supporting the notion that the rhythmic increases of sensitivity are independent of stimulus relevance <ref type="bibr" target="#b3">(Breska and Deouell, 2014)</ref>.</s><s>It is worth noting that, unlike in the previous electrophysiology studies <ref type="bibr" target="#b30">(Lakatos et al., 2013;</ref><ref type="bibr" target="#b45">O'Connell et al., 2014)</ref>, the perceptual discriminations here were based on chords with no overall frequency differences, showing that rhythm-induced expectations can work on composite representations.</s><s>It remains to be tested whether the rhythm-induced dynamic gain modulation generalizes across data modalities and species (e.g., invasive recordings in animal models).</s></p><p><s>On a methodological note, our study is the first to show robust M/EEG-based multivariate decoding of pure tone frequency across a broad range of frequencies.</s><s>While recent studies have brought substantial advances in decoding auditory features, studies using discreet stimuli have focused on decoding complex features such as pitch/rate modulation based on spectral information in MEG signals <ref type="bibr">(Herrmann et al., 2013b)</ref> or bistable percepts based on evoked MEG responses <ref type="bibr" target="#b1">(Billig et al., 2018)</ref>.</s><s>In the domain of speech decoding, speech-evoked responses can be used to decode vowel categories <ref type="bibr" target="#b67">(Yi et al., 2017)</ref>, but typically a combination of complex spectral features is used to decode the speech envelope <ref type="bibr" target="#b36">(Luo and Poeppel, 2007;</ref><ref type="bibr" target="#b42">Ng et al., 2013;</ref><ref type="bibr" target="#b11">de Cheveigne ´et al., 2018)</ref>.</s><s>Here, robust decoding of pure tone frequency was achieved based on relatively early M/EEG response latencies (Ͻ100 ms) evoked by very brief tones (ϳ33 ms), despite their presentation in gapless streams.</s><s>Finally, topographies of correlations between M/EEG amplitudes and tone frequency could be localized to auditory regions, suggesting that frequency decoding is based on sensory processing of acoustic features rather than on hierarchically higher activity related to complex percepts.</s></p><p><s>In summary, we have demonstrated that rhythmic expectation enhances population responses not only to task-relevant targets, but also to task-irrelevant distractors preceding potential targets.</s><s>The latter effect could be explained in terms of nonspecific neural gain changes at time points adjacent to rhythm-induced expectation of relevant latencies.</s><s>These findings speak against necessary temporal trade-offs in rhythmic orienting and support theories of neural alignment to the rhythmic structure of stimulus streams, plausibly mediated by dynamic neuromodulation.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc><div><p><s>Figure 1.</s><s>Behavioral paradigm and results.</s><s>A, Participants listened to sequences of pure tones interleaved with chords.</s><s>For simplicity, only chords (but not pure tones) are shown on the time axes.A subset of these chords (20%) had a markedly longer duration and constituted targets.</s><s>Upon hearing a target, participants were asked to categorize it as one of two predefined categories ("a" or "b") using a button press.</s><s>Sequences were presented in blocks of two experimental conditions, as follows: in the rhythmic condition, chords were presented with a fixed ISI of 1 s, and participants could form a temporal expectation of when to expect each upcoming chord.</s><s>In the jittered condition, half of the ISIs, chosen at random were fixed at 1 s, and the remaining half ranged between 0.5 and 1.5 s, making chord onset unpredictable.</s><s>B, Spectrograms of example trials including pure tones surrounding the chords.</s><s>C, Chords were composed of eight pure tones each: four discriminant frequencies (two with a higher amplitude for each chord) and four common frequencies with equal amplitude for both chords.</s><s>Pure tones were drawn from a larger set of 15 frequencies (range, 460 -1840 Hz), including frequencies constituting the chords and other frequencies not included in the chords (adjacent to the discriminant frequencies or distant from them).</s><s>D, E, Temporal expectation increased the participants' behavioral sensitivity (dЈ values) in the chord discrimination task (marked by asterisk), but did not significantly affect their reaction times.</s><s>Bars represent population means; solid (dashed) lines represent individual participants' data consistent (inconsistent) with the direction of the group effect; error bars denote the SEM.</s><s>n.s., not significant.</s></p></div></figDesc><graphic coords="3,107.00,63.54,372.00,318.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc><div><p><s>Figure2.</s><s>M/EEG sensor-level analysis.</s><s>A, Sensitivity to the amplitude of brief pure tones.</s><s>Time series of correlations between M/EEG amplitudes and pure tone frequency (shaded areas, SEM) for different channel types (black, EEG; cyan, MEG magnetometers; magenta, MEG planar gradiometers).</s><s>Each line represents the correlation coefficients between M/EEG amplitude and tone frequency, summarized across channels.</s><s>Horizontal bars mark cluster-corrected ( p Ͻ 0.05) significance against zero.</s><s>B, Topographies of the three respective correlation coefficients.</s><s>C, Orthogonal views of source estimates underlying the correlation peak, integrating across all channel types.</s><s>Source reconstruction of the correlation coefficient time series (based on the multiple sparse priors algorithm; see Materials and Methods) were estimated using data fused across channel types, and were inferred in the contrast between the time window of the observed correlations (26 -126 ms; chosen as a 100 ms time window around a peak correlation latency for all channel types) and the corresponding prestimulus baseline (126 -26 ms before tone onset).</s><s>All source estimates were significant at a threshold of p Ͻ 0.001 and correcting for multiple comparisons at a cluster level using an FWE-corrected p Ͻ 0.05.</s><s>Slices centered at 52, Ϫ48, and 6 mm in MNI coordinates.</s><s>D, Grand averages (shaded areas, SEM) of summarizedprincipalcomponentsoftone-evokedERP/ERFamplitudes,pertonefrequency(coloredlines)andchanneltype(panels).E,Grandaverages(shadedareas,SEM)ofsummarizedprincipalcomponents of chord-evoked ERP/ERF amplitudes, per condition (blue, rhythmic, red, jittered) and channel type (panels).</s><s>F, PLV: effect of rhythms (time-frequency maps).</s><s>Differences in PLV of M/EEG data at Ϫ500 to 500 ms relative to chord onset.</s><s>Each panel shows the time-frequency map of mean t statistics averaged across channels for a given channel type.</s><s>Contours outline the cluster of significant differences between rhythmic and jittered conditions, after correcting for multiple comparisons across channels and time-frequency points.</s><s>Asterisk: cluster-based p Ͻ 0.05; tilde: cluster-based p Ͻ 0.1.</s><s>G, PLV: effect of rhythms (topographic maps).</s><s>Each panel shows the topographical distribution of t statistic values at chord onset for the PLV estimate at 1 Hz.</s><s>Contours are as described above.</s></p></div></figDesc><graphic coords="5,83.00,63.42,420.00,292.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc><div><p><s>Figure 3. Decoding results.</s><s>A, Decoding methods were based on estimating multivariate Mahalanobis distance between M/EEG component amplitudes in a given (test) trial and average amplitudes calculated for all 15 frequencies, respectively (excluding the test trial).</s><s>The left panel presents M/EEG component amplitudes for two example components (empty circle, test trial; solid circles, ERPs/ERFs calculated from the remaining trials; acoustic frequencies are color coded).</s><s>Dashed lines on the left panel and bars on the right panel represent the multivariate distance between amplitudes observed in the test trial and the remaining trials.</s><s>B, Decoding methods as in A but for multiple components and multiple trials.</s><s>The left panel presents M/EEG component amplitudes (in columns) per trial (in rows), with the tone identity (1-15) presented on each trial noted on the left.</s><s>The middle panel presents the corresponding Mahalanobis distances per frequency (1-15, in columns) and trial (in rows).</s><s>Each row consists of a vector of distances between the neural activity on the given trial and the average neural activity in response to each of the 15 frequencies (calculated from all other trials; i.e., the single-trial dissimilarity estimates between amplitudes measured for the tone frequency presented in a given trial and all other frequencies presented in the remaining trials).</s><s>Frequency-tuning matrices (right), summarizing the population-level tuning curves, were obtained after averaging across trials, per frequency, resulting in a 15 ϫ 15 similarity matrix between all tone frequencies (each row represents the distance of all test trials of a given frequency to the remaining trials sorted per frequency and is shown in columns).</s><s>The observed frequency-tuning matrices (top right, example from one participant) were Spearman correlated with the "ideal" tuning matrix (bottom right), which consisted of the difference (in Hz) between pairs of tone frequencies.</s><s>This correlation coefficient provided a summary statistic that reflects decoding quality (i.e., how closely the relative dissimilarity between tone-evoked neural responses; "observed" in the figure) corresponds to the relative dissimilarity between tone frequencies ("ideal" in the figure).</s><s>C, The observed grand average frequency-tuning matrix (averaged across participants, time points, and conditions).</s><s>D, Rank-order correlation coefficients between the estimated tuning and ideal tuning for each frequency (i.e., each row in the frequency-tuning matrix).</s><s>Single grey dots mark single participants; black dots mark mean across participants.</s><s>E, Frequency decoding was significantly enhanced (cluster-corrected p Ͻ 0.05; black bar) in the rhythmic (blue) versus jittered (red) blocks between Ϫ100 and Ϫ80 ms before chord presentation.</s><s>Gray box marks chord presentation latency, where no pure tones were presented and consequently no frequency decoding can be established.</s><s>Shaded areas mark SEM across participants.</s><s>F, G, Chord decoding was based on the same methods as in A and B, except single-trial Mahalanobis distances were calculated for same versus different chords (instead of 15 different distractor frequencies).</s><s>Only neural responses to short chords preceded by an ISI of 1 s were analyzed.</s><s>H, Chord decoding was significantly enhanced (cluster-corrected p Ͻ 0.05; black bar) in the rhythmic (blue) vs jittered (red) blocks between 115 and 136 ms following chord onset.</s><s>Shaded areas mark the SEM across participants.</s><s>Freq, Frequency.</s></p></div></figDesc><graphic coords="6,107.00,63.34,372.00,290.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc><div><p><s>Figure 4. Modeling results.</s><s>A, Grand average frequency-tuning matrices for the rhythmic and jittered blocks, respectively (averaged between Ϫ100 and Ϫ80 ms before chord onset; Fig.3E).</s><s>Blue colors correspond to low distance (i.e., high similarity).</s><s>B, Effects of varying each of three free parameters in the gain/tuning model.</s><s>The x-axis corresponds to the off-diagonal and the y-axis to the shading of a frequency-tuning matrix.</s><s>C, Model comparison of seven models (solid vs no outline, free vs fixed gain; orange vs gray, free vs fixed tuning; dark vs light, free vs fixed constant).</s><s>The winning (full) model significantly outperformed the remaining models (see Results).</s><s>D, Effects of temporal expectation on model parameters.</s><s>Only the gain parameter was significantly different (asterisk) between rhythmic and jittered contexts.</s><s>E, Correlation between the benefit in tone decoding (for rhythmic vs jittered blocks) and the difference in gain parameters (between rhythmic and jittered conditions) of the model.</s><s>Dashed/solid line is the correlation coefficient slope before/after excluding an outlier (empty circle).</s><s>F, Relative gain (for rhythmic vs jittered conditions) did not significantly differ between models estimated separately for different frequency types (Fig.1C).</s><s>G, The time course of the gain parameters for the entire analyzed time range (Ϫ500 to 500 ms relative to chord onset).</s><s>Shaded areas mark SEMs.</s><s>Blue, Rhythmic blocks; red, jittered blocks.</s><s>Outline marks the latency of a significant effect reported in D. Rel., Relative.</s><s>n.s., not significant; *p Ͻ 0.05</s></p></div></figDesc><graphic coords="8,107.00,63.22,372.00,401.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . Source reconstruction of the topography of correlation between M/EEG amplitudes and tone frequency</head><label>1</label><figDesc></figDesc><table><row><cell>Cluster-level</cell><cell>Number</cell><cell>Peak-</cell><cell>Peak-</cell><cell cols="2">Peak MNI coordinates</cell><cell></cell><cell></cell></row><row><cell>p FWE-corr</cell><cell>of voxels</cell><cell>level T</cell><cell>level Z</cell><cell>x</cell><cell>y</cell><cell>z</cell><cell>Anatomical labels</cell></row><row><cell>0.005</cell><cell>4535</cell><cell>5.45</cell><cell>4.25</cell><cell></cell><cell>56 Ϫ38</cell><cell></cell><cell>6 Right MTG/STG</cell></row><row><cell></cell><cell></cell><cell>5.42</cell><cell>4.24</cell><cell></cell><cell>46 Ϫ32</cell><cell></cell><cell>4 Right STG/MTG/PT/TTG</cell></row><row><cell></cell><cell></cell><cell>5.06</cell><cell>4.05</cell><cell></cell><cell cols="3">48 Ϫ64 24 Right Ang/MOG/MTG</cell></row><row><cell>0.002</cell><cell>5215</cell><cell>5.15</cell><cell>4.10</cell><cell cols="4">Ϫ52 Ϫ44 30 Left SMG/PO/PT</cell></row><row><cell></cell><cell></cell><cell>5.14</cell><cell>4.09</cell><cell cols="4">Ϫ52 Ϫ44 10 Left STG/MTG/PT</cell></row><row><cell></cell><cell></cell><cell>5.03</cell><cell>4.03</cell><cell cols="4">Ϫ54 Ϫ26 24 Left PO/SMG/PoG/CO/PT</cell></row><row><cell></cell><cell></cell><cell>4.93</cell><cell>3.97</cell><cell cols="4">Ϫ52 Ϫ10 12 Left TTG/CO</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9808" xml:id="foot_0"><p><s>• J. Neurosci., December 4, 2019 • 39(49):9806 -9817 Auksztulewicz et al. • Temporal Expectation Increases Neural Gain</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9810" xml:id="foot_1"><p><s>• J. Neurosci., December 4, 2019 • 39(49):9806 -9817 Auksztulewicz et al. • Temporal Expectation Increases Neural Gain</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9812" xml:id="foot_2"><p><s>• J. Neurosci., December 4, 2019 • 39(49):9806 -9817 Auksztulewicz et al. • Temporal Expectation Increases Neural Gain</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p><s>MTG, Middle temporal gyrus; STG, superior temporal gyrus; PT, planum temporale; TTG, transverse temporal gyrus (Heschl's gyrus); Ang, angular gyrus; MOG, middle occipital gyrus; SMG, supramarginal gyrus; PO, parietal operculum; PoG, postcentralgyrus; CO, central operculum.</s><s>9814 • J. Neurosci., December 4, 2019 • 39(49):9806 -9817 Auksztulewicz et al. • Temporal Expectation Increases Neural Gain</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9816" xml:id="foot_4"><p><s>• J. Neurosci., December 4, 2019 • 39(49):9806 -9817 Auksztulewicz et al. • Temporal Expectation Increases Neural Gain</s></p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This work has been supported by the <rs type="funder">European Commission</rs><rs type="grantName">'s Marie Skłodowska-Curie Global Fellowship</rs> (<rs type="grantNumber">750459</rs> to R.A.) and the <rs type="funder">Wellcome Trust</rs> <rs type="grantName">Senior Investigator Award</rs> (<rs type="grantNumber">104571/Z/14/ Z</rs> to A.C.N.).We thank <rs type="person">Sven Braeutigam</rs>, <rs type="person">Sammi Chekroud</rs>, <rs type="person">Simone Heideman</rs>, and <rs type="person">Alex Irvine</rs> for help with data acquisition; as well as Freek van Ede, <rs type="person">Lucia Melloni</rs>, and <rs type="person">Vani Rajendran</rs> for useful discussions.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_vgzxQyb">
					<idno type="grant-number">750459</idno>
					<orgName type="grant-name">&apos;s Marie Skłodowska-Curie Global Fellowship</orgName>
				</org>
				<org type="funding" xml:id="_mvMM8YW">
					<idno type="grant-number">104571/Z/14/ Z</idno>
					<orgName type="grant-name">Senior Investigator Award</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Not all predictions are equal: &quot;what&quot; and &quot;when&quot; predictions modulate activity in auditory cortex through different mechanisms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Auksztulewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Schwiedrzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Thesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Devinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Nobre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Melloni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="8680" to="8693" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural decoding of bistable sounds reveals an effect of intention on perceptual organization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Billig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Carlyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2844" to="2853" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Measures of neural similarity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bobadilla-Suarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ahlheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Panos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Love</surname></persName>
		</author>
		<idno type="DOI">10.1101/439893</idno>
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint>
			<date type="published" when="2019-10-27">2019. October 27, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic bias of temporal expectations following temporally regular input independently of high-level temporal expectation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Breska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename><surname>Deouell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Cogn Neurosci</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1555" to="1571" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural mechanisms of rhythm-based temporal prediction: delta phase-locking reflects temporal predictability but not rhythmic entrainment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Breska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename><surname>Deouell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Biol</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2017">2017. 2001665</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Double dissociation of single-interval and rhythmic temporal prediction in cerebellar degeneration and Parkinson&apos;s disease</title>
		<author>
			<persName><forename type="first">A</forename><surname>Breska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Ivry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc Natl Acad Sci U S A</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="12283" to="12288" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visual attention: the past 25 years</title>
		<author>
			<persName><forename type="first">M</forename><surname>Carrasco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Res</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1484" to="1525" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The eyelink toolbox: eye tracking with MATLAB and the psychophysics toolbox</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W</forename><surname>Cornelissen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav Res Methods Instrum Comput</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="613" to="617" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Endogenous temporal orienting of attention in detection and discrimination tasks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lupia ´n ˜ez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Milliken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tudela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Percept Psychophys</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="264" to="278" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Selective entrainment of brain oscillations drives auditory perceptual organization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Costa-Faidella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Sussman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Escera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="195" to="206" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Temporal expectation enhances contrast sensitivity by phase entrainment of low-frequency oscillations in visual cortex</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Cravo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rohenkohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Wyart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Nobre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4002" to="4010" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Decoding the auditory brain with canonical component analysis</title>
		<author>
			<persName><forename type="first">´a</forename><surname>De Cheveigne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dde</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Liberto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Hjortkjaer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Slaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lalor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="206" to="216" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The mahalanobis distance</title>
		<author>
			<persName><forename type="first">R</forename><surname>De Maesschalck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jouan-Rimbaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Massart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemom Intell Lab Syst</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention flexibly trades off across points in time</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Denison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Heeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carrasco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychon Bull Rev</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1142" to="1151" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Near-real-time feature-selective modulations in human cortex</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Serences</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr Biol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="515" to="522" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A model of loudness applicable to timevarying sounds</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Glasberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bcj</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Audio Eng Soc</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="331" to="342" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Decoding dynamic brain patterns from evoked responses: a tutorial on multivariate pattern analysis applied to time series neuroimaging data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Grootswagers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Wardle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guggenmos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sterzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Cichy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Cogn Neurosci</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="434" to="447" />
			<date type="published" when="2017">2017. 2018</date>
		</imprint>
	</monogr>
	<note>Neuroimage</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rhythmic facilitation of sensory processing: a critical review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Haegens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zion</forename><surname>Golumbic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurosci Biobehav Rev</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="150" to="165" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Entrained neural oscillations in multiple frequency bands comodulate behavior</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Obleser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc Natl Acad Sci U S A</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="14935" to="14940" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MEG and EEG data fusion: simultaneous localisation o face-evoked responses</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Henson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mouchlianitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="581" to="589" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Frequency-specific adaptation in human auditory cortex depends on the spectral variance in the acoustic stimulation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Obleser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="2086" to="2096" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Oscillatory phase dynamics in neural entrainment underpin illusory percepts of time</title>
		<author>
			<persName><forename type="first">B</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grigutsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Obleser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="15799" to="15809" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal expectations and neural amplitude fluctuations in auditory cortex interactively influence perception</title>
		<author>
			<persName><forename type="first">B</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haegens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Obleser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="487" to="497" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Artifact correction of the ongoing EEG using spatial filters based on artefact and brain signal topographies</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scherg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Clin Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="113" to="124" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The auditory cortex mediates the perceptual effects of acoustic temporal expectation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jaramillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Zador</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Neurosci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="246" to="251" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Independent effects of bottom-up temporal expectancy and top-down spatial attention. an audiovisual study using rhythmic cueing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front Integr Neurosci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">96</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Applications of random field theory to electrophysiology</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kilner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Kiebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurosci Lett</title>
		<imprint>
			<biblScope unit="volume">374</biblScope>
			<biblScope unit="page" from="174" to="178" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Interpreting encoding and decoding models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kriegeskorte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Douglas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr Opin Neurobiol</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="167" to="179" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Measuring phase synchrony in brain signals</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martinerie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Varela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hum Brain Mapp</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="194" to="208" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Entrainment of neuronal oscillations as a mechanism of attentional selection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lakatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karmos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ulbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Schroeder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">320</biblScope>
			<biblScope unit="page" from="110" to="113" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The spectrotemporal filter mechanism of auditory selective attention</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lakatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Musacchia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Connel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Falchier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Javitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="750" to="761" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Temporal predictability enhances auditory detection</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Lawrance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Schnupp</surname></persName>
		</author>
		<idno>EL357-EL363</idno>
	</analytic>
	<monogr>
		<title level="j">J Acoust Soc Am</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Honey, I shrunk the sample covariance matrix</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ledoit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Portf Manag</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="110" to="119" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">How spatial and feature-based attention affect the gain and tuning of population responses</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carrasco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Res</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1194" to="1204" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Electromagnetic source reconstruction for group studies</title>
		<author>
			<persName><forename type="first">V</forename><surname>Litvak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1490" to="1498" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Algorithmic procedures for bayesian MEG/EEG source reconstruction in SPM</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lo ´pez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Litvak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Espinosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="476" to="487" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Phase patterns of neuronal responses reliably discriminate speech in human auditory cortex</title>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poeppel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1001" to="1010" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Nonparametric statistical testing of EEG-and MEG-data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Maris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Oostenveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci Methods</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Timing attention: cuing target onset interval attenuates the attentional blink</title>
		<author>
			<persName><forename type="first">S</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnson</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mem Cognit</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="234" to="240" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spectrotemporal receptive fields in the lemniscal auditory thalamus and cortex</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Escabí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Schreiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="516" to="527" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Temporal prediction in lieu of periodic stimulation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Morillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Wyart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Arnal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2342" to="2347" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Testing sensory evidence against mnemonic templates</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rohenkohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Wyart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Woolrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Nobre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Stokes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Elife 4:e09000</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">EEG phase patterns reflect the selectivity neural firing</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Logothetis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kayser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cereb Cortex</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="389" to="398" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Anticipated moments: temporal structure in attention</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Nobre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ede</forename><surname>Van</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Rev Neurosci</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="34" to="48" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">What do we talk about when we talk about rhythm?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Obleser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lakatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Biol</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">1002615</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Layer specific sharpening of frequency tuning by selective attention in primary auditory cortex</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>O'connell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lakatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="16496" to="16508" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bu ¨chel C (2012) Formal comparison of dual-parameter temporal discounting models in controls and pathological gamblers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Miedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">47225</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Human auditory sustained potentials. II. stimulus relationships</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Picton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Proulx</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electroencephalogr Clin Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="198" to="210" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Neurophysiology of implicit timing in serial choice reaction-time performance</title>
		<author>
			<persName><forename type="first">P</forename><surname>Praamstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kourtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Oostenveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="5448" to="5455" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Rhythm facilitates the detection of repeating sound patterns</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdel</forename><forename type="middle">-</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Schnupp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front Neurosci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Auditory target detection is affected by implicit temporal and spatial expectations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rimmele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jolsvai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sussman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Cogn Neurosci</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1136" to="1147" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">␣ oscillations related to anticipatory attention follow temporal expectations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rohenkohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Nobre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="14076" to="14084" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Behavioural dissociation between exogenous and endogenous temporal orienting of attention</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rohenkohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Coull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Nobre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14620</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Temporal expectation improves the quality of sensory information</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rohenkohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Cravo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Wyart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Nobre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8424" to="8428" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Combining spatial and temporal expectations to improve visual perception</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rohenkohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pessoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Nobre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Vis</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Rhythms that speed you up</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sanabria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Capizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Correa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Exp Psychol Hum Percept Perform</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="236" to="244" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Low-frequency neuronal oscillations as instruments of sensory selection</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lakatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Neurosci</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="9" to="18" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Attention to visual pattern information produces the attentional blink in rapid serial visual presentation</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Arnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Exp Psychol Hum Percept Perform</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="357" to="371" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Phase entrainment of human delta oscillations can mediate the effects of expectation on reaction speed</title>
		<author>
			<persName><forename type="first">G</forename><surname>Stefanics</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hangya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Herna ´di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lakatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ulbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="13578" to="13585" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Mapping tonotopic organization in human temporal cortex: representational similarity analysis in EMEG source space</title>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Zulfiqar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jamshed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fonteneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Marslen-Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front Neurosci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">368</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Perceptual encoding in auditory brainstem responses: effects of stimulus frequency</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Tabachnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Toscano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Speech Lang Hear Res</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="2364" to="2375" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Low-frequency cortical oscillations entrain to subthreshold rhythmic auditory stimuli</title>
		<author>
			<persName><forename type="first">Ten</forename><surname>Oever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Poeppel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Van Atteveldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Me ´gevand P, Groppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Zion-Golumbic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="4903" to="4912" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The caveats of observing inter-trial phase-coherence in cognitive neuroscience</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Van Diepen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mazaheri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">2990</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Decoding the influence of anticipatory states on visual perception in the presence of temporal distractors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Van Ede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Chekroud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Nobre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Commun</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">1449</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Reliability of dissimilarity measures for multi-voxel pattern analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Walther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ejaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kriegeskorte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Diedrichsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="page" from="188" to="200" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Dynamic hidden states underlying working-memory-guided behavior</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wolff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jochim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Akyu ¨rek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Stokes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Neurosci</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="864" to="871" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Middle latency auditory evoked potentials to tones of different frequency</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><forename type="middle">C</forename><surname>Covarrubias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zaidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hear Res</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="69" to="75" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Vowel decoding from single-trial speech-evoked electrophysiological responses: a feature-based machine learning approach</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Reetzke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chandrasekaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Behav</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">665</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Age-related changes in orienting attention in time</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Zanto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bollinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Nobre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gazzaley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="12461" to="12470" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Oscillatory mechanisms of stimulus processing and selection in the visual and auditory systems: state-of-the-art, speculations and suggestions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoefel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vanrullen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front Neurosci</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">296</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
